{
 "metadata": {
  "name": "",
  "signature": "sha256:ddef58c60f889b498d96ad929680bcce43c5050c0940231bf1645bbb404f6fab"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div align=\"center\">\n",
      "<h1>[Data, the Humanist's New Best Friend](index.ipynb)<br/>*Class 13*</h1>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this class you are expected to learn:\n",
      "\n",
      "* Regular expressions\n",
      "* Word inflection and lemmatization\n",
      "* Parsing\n",
      "* n-grams\n",
      "* Part-of-speech Tagging\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div align=\"center\">\n",
      "<img src=\"images/donothink.jpg\" width=\"350\">\n",
      "`n-grams`!\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Regular expressions\n",
      "-------------------\n",
      "\n",
      "The basics of regular expressions were covered in [class 7](class7.ipynb). But let's remember the most important.\n",
      "\n",
      "The power of regular expressions is that they can specify patterns, not just fixed characters. Here are the most basic patterns which match single chars:\n",
      "- `a`, `X`, `9` -- ordinary characters just match themselves exactly. The meta-characters which do not match themselves because they have special meanings are: `.` `^` `$` `*` `+` `?` `{` `[` `]` `\\` `|` `(` `)`\n",
      "- `.` (a period) -- matches any single character except newline `\\n`\n",
      "- `\\w` (lowercase w) -- matches a *word* character: a letter or digit or underbar `[a-zA-Z0-9_]`. Note that although \"word\" is the mnemonic for this, it only matches a single word char, not a whole word. \\W (upper case W) matches any non-word character.\n",
      "- `\\b` -- boundary between word and non-word\n",
      "- `\\s` -- (lowercase s) matches a single whitespace character -- space, newline, return, tab, form `[ \\n\\r\\t\\f]`. `\\S` (upper case S) matches any non-whitespace character.\n",
      "- `\\t`, `\\n`, `\\r` -- tab, newline, return\n",
      "- `\\d` -- decimal digit `[0-9]` (some older regex utilities do not support but `\\d`, but they all support `\\w` and `\\s`)\n",
      "- `^` = start, `$` = end -- match the start or end of the string\n",
      "- `\\` -- inhibit the \"specialness\" of a character. So, for example, use `\\.` to match a period or `\\\\` to match a slash. If you are unsure if a character has special meaning, such as `@`, you can put a slash in front of it, `\\@`, to make sure it is treated just as a character.\n",
      "\n",
      "The basic rules of regular expression search for a pattern within a string are:\n",
      "\n",
      "- The search proceeds through the string from start to end, stopping at the first match found\n",
      "- All of the pattern must be matched, but not all of the string\n",
      "- If `match = re.search(pat, str)` is successful, match is not `None` and in particular `match.group()` is the matching text\n",
      "\n",
      "Things get more interesting when you use `+` and `*` to specify repetition in the pattern\n",
      "- `+` -- 1 or more occurrences of the pattern to its left, e.g. `i+` = one or more i's\n",
      "- `*` -- 0 or more occurrences of the pattern to its left\n",
      "- `?` -- match 0 or 1 occurrences of the pattern to its left\n",
      "\n",
      "First the search finds the leftmost match for the pattern, and second it tries to use up as much of the string as possible -- i.e. `+` and `*` go as far as possible (the `+` and `*` are said to be \"greedy\").\n",
      "\n",
      "Square brackets can be used to indicate a set of chars, so `[abc]` matches `a` or `b` or `c`. The codes `\\w`, `\\s` etc. work inside square brackets too with the one exception that dot (`.`) just means a literal dot. For the emails addresses, the square brackets are an easy way to add `.` and `-` to the set of chars which can appear around the `@` with the pattern `r'[\\w.-]+@[\\w.-]+'` to get the whole email address.\n",
      "\n",
      "You can also use a dash to indicate a range, so `[a-z]` matches all lowercase letters. To use a dash without indicating a range, put the dash last, e.g. `[abc-]`. An up-hat (`^`) at the start of a square-bracket set inverts it, so [`^ab`] means any char except `'a'` or `'b'`.\n",
      "\n",
      "The *group* feature of a regular expression allows you to pick out parts of the matching text. Suppose for the emails problem that we want to extract the username and host separately. To do this, add parenthesis `(` `)` around the username and host in the pattern, like this: `r'([\\w.-]+)@([\\w.-]+)'`. In this case, the parenthesis do not change what the pattern will match, instead they establish logical \"groups\" inside of the match text. On a successful search, `match.group(1)` is the match text corresponding to the 1st left parenthesis, and `match.group(2)` is the text corresponding to the 2nd left parenthesis. The plain `match.group()` is still the whole match text as usual.\n",
      "\n",
      "If you still have problems visualizing the actual diagram that is being executed when recognizing a regular expression, just go to <a href=\"http://regexper.com/#(%5B%5Cw%5C.-%5D%2B)%40(%5B%5Cw%5C.-%5D%2B)\">regexper</a> and play with some examples."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Part-of-Speech Tagging\n",
      "----------------------\n",
      "\n",
      "There are some cases when regular expressions are not enough for recognizing or extracting information. Let's say that we have a text and we want to extract all uses of the verb \"to be\", in all its present inflections. Our regular expression would look seomthing like:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "to_be = re.compile(r\"(am|is|are)\")\n",
      "text = \"All your base are belong to us\"\n",
      "to_be.findall(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 1,
       "text": [
        "['are']"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What about contractions?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "to_be = re.compile(r\"(am|is|are|'s|'re|'m)\")\n",
      "text = \"All your base are belong to us. I'm not the jedi you're looking for.\"\n",
      "to_be.findall(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "['are', \"'m\", \"'re\"]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And that might work sometimes, but in other cases, for example when using possesives, it doesn't."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "to_be = re.compile(r\"(am|is|are|'s|'re|'m)\")\n",
      "text = \"I'll stay at Ben's\"\n",
      "to_be.findall(text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[\"'s\"]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And the reason is that in those examples `'s` has different purposes. In other words, the same word can play a different part in the speech. That's the reason that many times we need to tag the part-of-speech in order to remove ambiguity. Some useful usage includes:\n",
      "\n",
      "- Information Retrieval\n",
      "- Text to Speech: `object(N)` vs. `object(V)`, or `discount(N)` vs. `discount(V)`\n",
      "- Word sense disambiguation\n",
      "- As a preprocessing step of parsing\n",
      "- Unique tag to each word reduces the number of parses \n",
      "\n",
      "The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech tagging, POS-tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories. The collection of tags used for a particular task is known as a tagset. A very commonly used set is the [University of Pennsylvania TreeBank II tagset](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html), and it has 36 word tags (plus *.,:()* for punctuation marks).\n",
      "\n",
      "<table cellpadding=\"2\" cellspacing=\"2\" border=\"0\">\n",
      "  <tr bgcolor=\"#DFDFFF\" align=\"none\"> \n",
      "    <td align=\"none\"> \n",
      "      <div align=\"left\">Number</div>\n",
      "    </td>\n",
      "    <td> \n",
      "      <div align=\"left\">Tag</div>\n",
      "    </td>\n",
      "    <td> \n",
      "      <div align=\"left\">Description</div>\n",
      "    </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 1. </td>\n",
      "    <td>CC </td>\n",
      "    <td>Coordinating conjunction </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 2. </td>\n",
      "    <td>CD </td>\n",
      "    <td>Cardinal number </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 3. </td>\n",
      "    <td>DT </td>\n",
      "    <td>Determiner </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 4. </td>\n",
      "    <td>EX </td>\n",
      "    <td>Existential <i>there<i> </i></i></td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 5. </td>\n",
      "    <td>FW </td>\n",
      "    <td>Foreign word </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 6. </td>\n",
      "    <td>IN </td>\n",
      "    <td>Preposition or subordinating conjunction </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 7. </td>\n",
      "    <td>JJ </td>\n",
      "    <td>Adjective </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 8. </td>\n",
      "    <td>JJR </td>\n",
      "    <td>Adjective, comparative </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 9. </td>\n",
      "    <td>JJS </td>\n",
      "    <td>Adjective, superlative </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 10. </td>\n",
      "    <td>LS </td>\n",
      "    <td>List item marker </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 11. </td>\n",
      "    <td>MD </td>\n",
      "    <td>Modal </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 12. </td>\n",
      "    <td>NN </td>\n",
      "    <td>Noun, singular or mass </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 13. </td>\n",
      "    <td>NNS </td>\n",
      "    <td>Noun, plural </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 14. </td>\n",
      "    <td>NNP </td>\n",
      "    <td>Proper noun, singular </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 15. </td>\n",
      "    <td>NNPS </td>\n",
      "    <td>Proper noun, plural </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 16. </td>\n",
      "    <td>PDT </td>\n",
      "    <td>Predeterminer </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 17. </td>\n",
      "    <td>POS </td>\n",
      "    <td>Possessive ending </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 18. </td>\n",
      "    <td>PRP </td>\n",
      "    <td>Personal pronoun </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 19. </td>\n",
      "    <td>PRP\\$ </td>\n",
      "    <td>Possessive pronoun </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 20. </td>\n",
      "    <td>RB </td>\n",
      "    <td>Adverb </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 21. </td>\n",
      "    <td>RBR </td>\n",
      "    <td>Adverb, comparative </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 22. </td>\n",
      "    <td>RBS </td>\n",
      "    <td>Adverb, superlative </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 23. </td>\n",
      "    <td>RP </td>\n",
      "    <td>Particle </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 24. </td>\n",
      "    <td>SYM </td>\n",
      "    <td>Symbol </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 25. </td>\n",
      "    <td>TO </td>\n",
      "    <td><i>to</i> </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 26. </td>\n",
      "    <td>UH </td>\n",
      "    <td>Interjection </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 27. </td>\n",
      "    <td>VB </td>\n",
      "    <td>Verb, base form </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 28. </td>\n",
      "    <td>VBD </td>\n",
      "    <td>Verb, past tense </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 29. </td>\n",
      "    <td>VBG </td>\n",
      "    <td>Verb, gerund or present participle </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 30. </td>\n",
      "    <td>VBN </td>\n",
      "    <td>Verb, past participle </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 31. </td>\n",
      "    <td>VBP </td>\n",
      "    <td>Verb, non-3rd person singular present </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 32. </td>\n",
      "    <td>VBZ </td>\n",
      "    <td>Verb, 3rd person singular present </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 33. </td>\n",
      "    <td>WDT </td>\n",
      "    <td>Wh-determiner </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 34. </td>\n",
      "    <td>WP </td>\n",
      "    <td>Wh-pronoun </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 35. </td>\n",
      "    <td>WP\\$ </td>\n",
      "    <td>Possessive wh-pronoun </td>\n",
      "  </tr>\n",
      "  <tr bgcolor=\"#FFFFCA\"> \n",
      "    <td align=\"none\"> 36. </td>\n",
      "    <td>WRB </td>\n",
      "    <td>Wh-adverb \n",
      "</table>\n",
      "\n",
      "Another set often used is the so called universal tagset, which NLTK uses by default.\n",
      "\n",
      "<table border=\"1\" class=\"docutils\" id=\"tab-universal-tagset\">\n",
      "<colgroup>\n",
      "<col width=\"11%\">\n",
      "<col width=\"27%\">\n",
      "<col width=\"62%\">\n",
      "</colgroup>\n",
      "<thead valign=\"bottom\">\n",
      "<tr><th class=\"head\">Tag</th>\n",
      "<th class=\"head\">Meaning</th>\n",
      "<th class=\"head\">English Examples</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody valign=\"top\">\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">ADJ</span></tt></td>\n",
      "<td>adjective</td>\n",
      "<td><span class=\"example\">new, good, high, special, big, local</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">ADP</span></tt></td>\n",
      "<td>adposition</td>\n",
      "<td><span class=\"example\">on, of, at, with, by, into, under</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">ADV</span></tt></td>\n",
      "<td>adverb</td>\n",
      "<td><span class=\"example\">really, already, still, early, now</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">CONJ</span></tt></td>\n",
      "<td>conjunction</td>\n",
      "<td><span class=\"example\">and, or, but, if, while, although</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">DET</span></tt></td>\n",
      "<td>determiner, article</td>\n",
      "<td><span class=\"example\">the, a, some, most, every, no, which</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">NOUN</span></tt></td>\n",
      "<td>noun</td>\n",
      "<td><span class=\"example\">year, home, costs, time, Africa</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">NUM</span></tt></td>\n",
      "<td>numeral</td>\n",
      "<td><span class=\"example\">twenty-four, fourth, 1991, 14:24</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">PRT</span></tt></td>\n",
      "<td>particle</td>\n",
      "<td><span class=\"example\">at, on, out, over per, that, up, with</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">PRON</span></tt></td>\n",
      "<td>pronoun</td>\n",
      "<td><span class=\"example\">he, their, her, its, my, I, us</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">VERB</span></tt></td>\n",
      "<td>verb</td>\n",
      "<td><span class=\"example\">is, say, told, given, playing, would</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">.</span></tt></td>\n",
      "<td>punctuation marks</td>\n",
      "<td><span class=\"example\">. , ; !</span></td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">X</span></tt></td>\n",
      "<td>other</td>\n",
      "<td><span class=\"example\">ersatz, esprit, dunno, gr8, univeristy</span></td>\n",
      "</tr>\n",
      "</tbody>\n",
      "\n",
      "\n",
      "</table>\n",
      "\n",
      "There are different techniques for POS tagging, but exploiting tags and tagging text automatically is what we are looking for.\n",
      "\n",
      "For example, in our previous example `\"I'll stay at Ben's\"`, we want to know if `'s` is referring to the possesive or to the singular $1^{st}$ person of the imperfective indicative.\n",
      "\n",
      "TextBlob POS tagger is based on NTLK and parser, and it uses a specific encoding for the tags."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob import TextBlob\n",
      "TextBlob(\"I'll stay at Ben's\").tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[('I', 'PRP'),\n",
        " (\"'\", 'POS'),\n",
        " ('ll', 'NN'),\n",
        " ('stay', 'VB'),\n",
        " ('at', 'IN'),\n",
        " ('Ben', 'NNP'),\n",
        " (\"'\", 'POS'),\n",
        " ('s', 'PRP')]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we see, `s` is tagged with `PRP`, and from the table above `PRP` stands for personal pronoun. Therefore, in this case, we can say that `'s` is acting as a pronoun rather than a verb."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(\"He's not leaving\").tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "[('He', 'PRP'), (\"'\", 'POS'), ('s', 'PRP'), ('not', 'RB'), ('leaving', 'VBG')]"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the last example, however, the tagger is saying that `s` is acting as pronoun, when we know that is justa verb. Let's see how NLTK tagger behaves."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "tokens = nltk.word_tokenize(\"He's not leaving\")\n",
      "nltk.pos_tag(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "[('He', 'PRP'), (\"'s\", 'VBZ'), ('not', 'RB'), ('leaving', 'VBG')]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK tagger is correctly saying that `'s` is a verb. In fact, is a non-3rd person singular present. That happens because by default `TextBlob` uses a different tagger, `PatternTagger`, based on a library called [`pattern`](http://www.clips.ua.ac.be/pages/pattern-en#parser). However, `TextBlob` can be set to use the tagger included in NLTK, which usually performs a bit better."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob.taggers import NLTKTagger\n",
      "nltk_tagger = NLTKTagger()\n",
      "blob = TextBlob(\"He's not leaving\", pos_tagger=nltk_tagger)\n",
      "blob.pos_tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[('He', 'PRP'), (\"'s\", 'VBZ'), ('not', 'RB'), ('leaving', 'VBG')]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
      "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
      "Activity\n",
      "</p>\n",
      "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
      "Find out which tagger works better for the next sentences. Explain why and extract nouns, verbs and adjectives.\n",
      "<ul><li>\n",
      "One morning I shot an elephant in my pyjamas\n",
      "</li><li>\n",
      "John saw the man on the mountain with a telescope\n",
      "</li><li>\n",
      "The word of the Lord came to Zechariah, son of Berekiah, son of Iddo, the prophet\n",
      "</li><li>\n",
      "The pet of the woman that had the parasol was brown\n",
      "</li><li>\n",
      "The dog brings me the newspaper every morning\n",
      "</li></ul>\n",
      "<!-- <small style=\"float: right; position: relative;\">[Solution](data/solution.py)</small> -->\n",
      "</p>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Chunking\n",
      "--------\n",
      "\n",
      "Another level of analysis is extracting the bigger chunks that tags are included into. It's the basic technique we will use for entity detection. Chunking segments and labels multi-token sequences. The smaller boxes show the word-level tokenization and part-of-speech tagging, while the large boxes show higher-level chunking. Each of these larger boxes is called a chunk. Like tokenization, which omits whitespace, chunking usually selects a subset of the tokens. Also like tokenization, the pieces produced by a chunker do not overlap in the source text.\n",
      "\n",
      "<div align=\"center\">\n",
      "<img src=\"images/chunk-segmentation.png\" width=\"500\">\n",
      "Segmentation and Labeling at both the Token and Chunk Levels\n",
      "</div>\n",
      "\n",
      "We will begin by considering the task of noun phrase chunking, or NP-chunking, where we search for chunks corresponding to individual noun phrases. For example, here is some Wall Street Journal text with NP-chunks marked using brackets:\n",
      "\n",
      "    [ The/DT market/NN ] for/IN [ system-management/NN software/NN ] for/IN [ Digital/NNP ] [ 's/POS hardware/NN ] is/VBZ fragmented/JJ enough/RB that/IN [ a/DT giant/NN ] such/JJ as/IN [ Computer/NNP Associates/NNPS ] should/MD do/VB well/RB there/RB ./.\n",
      "\n",
      "As we can see, NP-chunks are often smaller pieces than complete noun phrases. For example, `the market for system-management software for Digital's hardware` is a single noun phrase (containing two nested noun phrases), but it is captured in NP-chunks by the simpler chunk the market. One of the motivations for this difference is that NP-chunks are defined so as not to contain other NP-chunks. Consequently, any prepositional phrases or subordinate clauses that modify a nominal will not be included in the corresponding NP-chunk, since they almost certainly contain further noun phrases.\n",
      "\n",
      "One of the most useful sources of information for NP-chunking is part-of-speech tags. This is one of the motivations for performing part-of-speech tagging in our information extraction system. We demonstrate this approach using an example sentence that has been part-of-speech tagged. In order to create an NP-chunker, we will first define a chunk grammar, consisting of rules that indicate how sentences should be chunked. In this case, we will define a simple grammar with a single regular-expression rule. This rule says that an NP chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN). Using this grammar, we create a chunk parser, and test it on our example sentence. The result is a tree, which we can either print, or display graphically."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
      "            (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
      "\n",
      "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
      "\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "result = cp.parse(sentence)\n",
      "print(result)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
        "  barked/VBD\n",
        "  at/IN\n",
        "  (NP the/DT cat/NN))\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The rules that make up a chunk grammar use tag patterns to describe sequences of tagged words. A tag pattern is a sequence of part-of-speech tags delimited using angle brackets, e.g. `<DT>?<JJ>*<NN>`. Tag patterns are similar to regular expression patterns. Now, consider the following noun phrases from the Wall Street Journal:\n",
      "\n",
      "    another/DT sharp/JJ dive/NN\n",
      "    trade/NN figures/NNS\n",
      "    any/DT new/JJ policy/NN measures/NNS\n",
      "    earlier/JJR stages/NNS\n",
      "    Panamanian/JJ dictator/NN Manuel/NNP Noriega/NNP\n",
      "    \n",
      "To find the chunk structure for a given sentence, the `RegexpParser` chunker begins with a flat structure in which no tokens are chunked. The chunking rules are applied in turn, successively updating the chunk structure. Once all of the rules have been invoked, the resulting chunk structure is returned.\n",
      "\n",
      "In the next example, the first rule matches an optional determiner or possessive pronoun, zero or more adjectives, then a noun. The second rule matches one or more proper nouns. We also define an example sentence to be chunked, and run the chunker on this input."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = r\"\"\"\n",
      "  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
      "      {<NNP>+}                # chunk sequences of proper nouns\n",
      "\"\"\"\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "sentence = [(\"Rapunzel\", \"NNP\"), (\"let\", \"VBD\"), (\"down\", \"RP\"),\n",
      "                 (\"her\", \"PP$\"), (\"long\", \"JJ\"), (\"golden\", \"JJ\"), (\"hair\", \"NN\")]\n",
      "\n",
      "print(cp.parse(sentence))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP Rapunzel/NNP)\n",
        "  let/VBD\n",
        "  down/RP\n",
        "  (NP her/PP$ long/JJ golden/JJ hair/NN))\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sometimes it is easier to define what we want to exclude from a chunk. We can define a chink to be a sequence of tokens that is not included in a chunk. In the following example, barked/VBD at/IN is a chink."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "grammar = r\"\"\"\n",
      "  NP:\n",
      "    {<.*>+}          # Chunk everything\n",
      "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
      "  \"\"\"\n",
      "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
      "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
      "cp = nltk.RegexpParser(grammar)\n",
      "print(cp.parse(sentence))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(S\n",
        "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
        "  barked/VBD\n",
        "  at/IN\n",
        "  (NP the/DT cat/NN))\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As befits their intermediate status between tagging and parsing, chunk structures can be represented using either tags or trees. The most widespread file representation uses IOB tags. In this scheme, each token is tagged with one of three special chunk tags, `I` (inside), `O` (outside), or `B` (begin). A token is tagged as `B` if it marks the beginning of a chunk. Subsequent tokens within the chunk are tagged `I`. All other tokens are tagged `O`. The `B` and `I` tags are suffixed with the chunk type, e.g. `B-NP`, `I-NP`. Of course, it is not necessary to specify a chunk type for tokens that appear outside a chunk, so these are just labeled `O`.\n",
      "\n",
      "\n",
      "<div align=\"center\">\n",
      "<br/>\n",
      "<img src=\"images/chunk-tagrep.png\" width=\"500\">\n",
      "Tag Representation of Chunk Structures\n",
      "</div>\n",
      "\n",
      "IOB tags have become the standard way to represent chunk structures in files, and we will also be using this format.\n",
      "\n",
      "    We PRP B-NP\n",
      "    saw VBD O\n",
      "    the DT B-NP\n",
      "    yellow JJ I-NP\n",
      "    dog NN I-NP\n",
      "\n",
      "In this representation there is one token per line, each with its part-of-speech tag and chunk tag. This format permits us to represent more than one chunk type, so long as the chunks do not overlap. As we saw earlier, chunk structures can also be represented using trees. These have the benefit that each chunk is a constituent that can be manipulated directly.\n",
      "\n",
      "<div align=\"center\">\n",
      "<br/>\n",
      "<img src=\"images/chunk-treerep.png\" width=\"500\">\n",
      "Tree Representation of Chunk Structures\n",
      "</div>\n",
      "\n",
      "However fun can be to create our own chunkers and taggers, NLTK and TextBlob do that for us. TextBlob even has already a handy method to extract noun phrases."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b = TextBlob(\"Leela save me! And my Banjo! And Fry! And yourself I guess!\")\n",
      "print(b.parse())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Leela/NNP/B-NP/O save/VB/B-VP/O me/PRP/B-NP/O !/./O/O\n",
        "And/CC/O/O my/PRP$/B-NP/O Banjo/NNP/I-NP/O !/./O/O\n",
        "And/CC/O/O Fry/VB/B-VP/O !/./O/O\n",
        "And/CC/O/O yourself/PRP/B-NP/O I/PRP/I-NP/O guess/VBP/B-VP/O !/./O/O\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "b.noun_phrases"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "WordList(['leela', 'banjo', 'fry'])"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
      "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
      "Activity\n",
      "</p>\n",
      "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
      "`TextBlob` also has a sentence tokenizer. Just by invoking `.sentences` on a `TextBlob` object. For the text in [this article](http://tmagazine.blogs.nytimes.com/2015/03/06/ordos-china-tourist-city/?hp&action=click&pgtype=Homepage&module=mini-moth&region=top-stories-below&WT.nav=top-stories-below&_r=0), split it into sentences and extract noun phrases and verbs. For each, plot two charts, one containing the 10 most frequent noun phrases, and the other the 10 most frequent verbs.\n",
      "<!-- <small style=\"float: right; position: relative;\">[Solution](data/solution.py)</small> -->\n",
      "</p>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "N-grams\n",
      "-------\n",
      "\n",
      "Other useful methods of `TextBlob` include extrating of n-grams, lemmatization and word inflection. An n-gram is a contiguous sequence of n items from a given sequence of text. The items can be phonemes, syllables, letters, words or base pairs according, although in our case are just words.\n",
      "\n",
      "An n-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"bigram\" (or, less commonly, a \"digram\"); size 3 is a \"trigram\". Larger sizes are sometimes referred to by the value of n, e.g., \"four-gram\", \"five-gram\", and so on.\n",
      "\n",
      "Beyond computational linguistics applications, such as statistical natural language processing, n-gram models are now widely used in probability, communication theory, computational biology, and data compression. There are even methods for authorship attribution of anonymous texts based on n-grams analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(\"Now is better than never. Now is better than ever.\").ngrams(n=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "[WordList(['Now', 'is', 'better']),\n",
        " WordList(['is', 'better', 'than']),\n",
        " WordList(['better', 'than', 'never']),\n",
        " WordList(['than', 'never', 'Now']),\n",
        " WordList(['never', 'Now', 'is']),\n",
        " WordList(['Now', 'is', 'better']),\n",
        " WordList(['is', 'better', 'than']),\n",
        " WordList(['better', 'than', 'ever'])]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Word inflection and lemmatization\n",
      "---------------------------------\n",
      "\n",
      "Each word in `TextBlob.words` or `Sentence.words` is a `Word` object (a subclass of `unicode`) with useful methods, e.g. for word inflection."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence = TextBlob('Use 4 spaces per indentation level.')\n",
      "sentence.words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "WordList(['Use', '4', 'spaces', 'per', 'indentation', 'level'])"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence.words[2].singularize()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "'space'"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sentence.words[-1].pluralize()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "'levels'"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Words can be lemmatized by calling the lemmatize method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob import Word\n",
      "w = Word(\"octopi\")\n",
      "w.lemmatize()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "'octopus'"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "w = Word(\"went\")\n",
      "w.lemmatize(\"v\")  # Pass in part of speech (verb)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "'go'"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "For the [next class](classYY.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Read [Chapters 7-9 and 13-14, from Statistics for Humanities](http://statisticsforhumanities.net/).\n",
      "- Read [Ordinary Least Squares in Python, from datarobot](http://www.datarobot.com/blog/ordinary-least-squares-in-python/).\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}