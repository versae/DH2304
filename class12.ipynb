{
 "metadata": {
  "name": "",
  "signature": "sha256:83b4ec82d0a6d805287fc413cf9a45a6091755ccd47ef878a7e932751d8a01b7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div align=\"center\">\n",
      "<h1>[Data, the Humanist's New Best Friend](index.ipynb)<br/>*Class 12*</h1>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this class you are expected to learn:\n",
      "\n",
      "* Corpora\n",
      "* Conditional Frequency Distributions\n",
      "* Sources of data\n",
      "* Language detection\n",
      "* Machine translation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div align=\"center\">\n",
      "<img src=\"files/images/morfeo.jpg\" width=\"350\">\n",
      "*Morfeo, always kidding*\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Corpora"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK is distributed with corpora that can be accessed using `nltk.corpus` package. First we import the Brown Corpus, the oldest million-word, part-of-speech tagged (stuff like subject, verb, etc.) electronic corpus of English. Each of the sections represents a different genre, or category of text. A list of these categories can be accessed using `categories()`, and a list of the files in each section can be accessed using `fileids()`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from nltk.corpus import brown"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown.fileids()[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "['ca01',\n",
        " 'ca02',\n",
        " 'ca03',\n",
        " 'ca04',\n",
        " 'ca05',\n",
        " 'ca06',\n",
        " 'ca07',\n",
        " 'ca08',\n",
        " 'ca09',\n",
        " 'ca10']"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown.categories()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "['adventure',\n",
        " 'belles_lettres',\n",
        " 'editorial',\n",
        " 'fiction',\n",
        " 'government',\n",
        " 'hobbies',\n",
        " 'humor',\n",
        " 'learned',\n",
        " 'lore',\n",
        " 'mystery',\n",
        " 'news',\n",
        " 'religion',\n",
        " 'reviews',\n",
        " 'romance',\n",
        " 'science_fiction']"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown.fileids(['adventure', 'romance'])[:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "['cn01',\n",
        " 'cn02',\n",
        " 'cn03',\n",
        " 'cn04',\n",
        " 'cn05',\n",
        " 'cn06',\n",
        " 'cn07',\n",
        " 'cn08',\n",
        " 'cn09',\n",
        " 'cn10']"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown.categories(['ca01', 'cp28'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "['news', 'romance']"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The methods `words()` and `sents()` provide access to the corpus as a list of words, or a list of sentences, respectively."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown.words('ca01')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown.sents('ca01')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown.sents('ca01')[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "['The',\n",
        " 'Fulton',\n",
        " 'County',\n",
        " 'Grand',\n",
        " 'Jury',\n",
        " 'said',\n",
        " 'Friday',\n",
        " 'an',\n",
        " 'investigation',\n",
        " 'of',\n",
        " \"Atlanta's\",\n",
        " 'recent',\n",
        " 'primary',\n",
        " 'election',\n",
        " 'produced',\n",
        " '``',\n",
        " 'no',\n",
        " 'evidence',\n",
        " \"''\",\n",
        " 'that',\n",
        " 'any',\n",
        " 'irregularities',\n",
        " 'took',\n",
        " 'place',\n",
        " '.']"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "brown.sents(brown.fileids(['adventure']))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...]"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
      "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
      "Activity\n",
      "</p>\n",
      "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
      "Write a function, `word_freq()`, that takes a word and the name of a section of the Brown Corpus as arguments, and computes the frequency of the word in that section of the corpus.\n",
      "</p>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But there are many many more corpora in NLTK, take a look to the [complete list](http://www.nltk.org/nltk_data/). Another interesting corpus is the Project Gutenberg, although NLTK does not include the whole collection of about 25,000 books."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import Text\n",
      "from nltk.corpus import gutenberg\n",
      "Text(gutenberg.raw('melville-moby_dick.txt'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "<Text: [ M o b y   D i...>"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
      "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
      "Activity\n",
      "</p>\n",
      "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
      "Write a program to create a table of word frequencies by genre. Choose your own words and try to find words whose presence (or absence) is typical of a genre. Discuss your findings with your partner.\n",
      "</p>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Conditional Frequency Distributions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The adventage of having your data as a corpus is that you can generalize the idea of frequency distributions. When the texts of a corpus are divided into several categories, by genre, topic, author, etc, we can maintain separate frequency distributions for each category. This will allow us to study systematic differences between the categories. We achieve this using NLTK's `ConditionalFreqDist` data type. A conditional frequency distribution is a collection of frequency distributions, each one for a different \"condition\". The condition will often be the category of the text.\n",
      "\n",
      "A frequency distribution counts observable events, such as the appearance of words in a text. A conditional frequency distribution needs to pair each event with a condition. So instead of processing a sequence of words, we have to process a sequence of pairs.\n",
      "```python\n",
      "text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
      "pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]\n",
      "```\n",
      "Each pair has the form (condition, event). If we were processing the entire Brown Corpus by genre there would be 15 conditions (one per genre), and 1,161,192 events (one per word).\n",
      "\n",
      "So `ConditionalFreqDist()` takes a list of pairs."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import brown\n",
      "cfd = nltk.ConditionalFreqDist(\n",
      "          (genre, word)\n",
      "          for genre in brown.categories()\n",
      "          for word in brown.words(categories=genre))\n",
      "cfd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<ConditionalFreqDist with 15 conditions>"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's break this down, and look at just two genres, news and romance. For each genre [2], we loop over every word in the genre [3], producing pairs consisting of the genre and the word [1]:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "genre_word = [(genre, word)  # [1]\n",
      "              for genre in ['news', 'romance']  # [2]\n",
      "              for word in brown.words(categories=genre)]  # [3]\n",
      "len(genre_word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "170576"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, as we can see below, pairs at the beginning of the list genre_word will be of the form ('news', word), while those at the end will be of the form ('romance', word)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "genre_word[:4]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "[('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ('news', 'Grand')]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "genre_word[-4:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "[('romance', 'afraid'),\n",
        " ('romance', 'not'),\n",
        " ('romance', \"''\"),\n",
        " ('romance', '.')]"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can now use this list of pairs to create a ConditionalFreqDist, and save it in a variable cfd. As usual, we can type the name of the variable to inspect it [1], and verify it has two conditions [2]:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd = nltk.ConditionalFreqDist(genre_word)\n",
      "cfd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 15,
       "text": [
        "<ConditionalFreqDist with 2 conditions>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd.conditions()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "['news', 'romance']"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's access the two conditions, and satisfy ourselves that each is just a frequency distribution."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd['news']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "FreqDist({'the': 5580, ',': 5188, '.': 4030, 'of': 2849, 'and': 2146, 'to': 2116, 'a': 1993, 'in': 1893, 'for': 943, 'The': 806, ...})"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd['romance']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "FreqDist({',': 3899, '.': 3736, 'the': 2758, 'and': 1776, 'to': 1502, 'a': 1335, 'of': 1186, '``': 1045, \"''\": 1044, 'was': 993, ...})"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "list(cfd['romance'])[:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "['canoe', 'mares', 'shoving', 'shape-up', 'Turning']"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cfd['romance']['could']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "193"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As a cheat sheet, here are some of the most commonly used methods. We haven't seen yet `plot()` or `tabulate()`, though.\n",
      "\n",
      "<table border=\"1\" class=\"docutils\" id=\"tab-conditionalfreqdist\">\n",
      "<colgroup>\n",
      "<col width=\"36%\">\n",
      "<col width=\"64%\">\n",
      "</colgroup>\n",
      "<thead valign=\"bottom\">\n",
      "<tr><th class=\"head\">Example</th>\n",
      "<th class=\"head\">Description</th>\n",
      "</tr>\n",
      "</thead>\n",
      "<tbody valign=\"top\">\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">cfdist = ConditionalFreqDist(pairs)</span></tt></td>\n",
      "<td>create a conditional frequency distribution from a list of pairs</td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">cfdist.conditions()</span></tt></td>\n",
      "<td>alphabetically sorted list of conditions</td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">cfdist[condition]</span></tt></td>\n",
      "<td>the frequency distribution for this condition</td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">cfdist[condition][sample]</span></tt></td>\n",
      "<td>frequency for the given sample for this condition</td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">cfdist.tabulate()</span></tt></td>\n",
      "<td>tabulate the conditional frequency distribution</td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">cfdist.tabulate(samples, conditions)</span></tt></td>\n",
      "<td>tabulation limited to the specified samples and conditions</td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">cfdist.plot()</span></tt></td>\n",
      "<td>graphical plot of the conditional frequency distribution</td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">cfdist.plot(samples, conditions)</span></tt></td>\n",
      "<td>graphical plot limited to the specified samples and conditions</td>\n",
      "</tr>\n",
      "<tr><td><tt class=\"doctest\"><span class=\"pre\">cfdist1 &lt; cfdist2</span></tt></td>\n",
      "<td>test if samples in <tt class=\"doctest\"><span class=\"pre\">cfdist1</span></tt> occur less frequently than in <tt class=\"doctest\"><span class=\"pre\">cfdist2</span></tt></td>\n",
      "</tr>\n",
      "</tbody>\n",
      "\n",
      "\n",
      "</table>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
      "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
      "Activity\n",
      "</p>\n",
      "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
      "Zipf's Law: Let $f(w)$ be the frequency of a word $w$ in free text. Suppose that all the words of a text are ranked according to their frequency, with the most frequent word first. Zipf's law states that the frequency of a word type is inversely proportional to its rank (i.e. $f \u00d7 r = k$, for some constant $k$). For example, the 50th most common word type should occur three times as frequently as the 150th most common word type.\n",
      "<br/><br/>\n",
      "Write a function to process a large text and plot word frequency against word rank using regular `matplotlib` plot. Do you confirm Zipf's law? (*Hint*: it helps to use a logarithmic scale). What is going on at the extreme ends of the plotted line?\n",
      "<br/><br/>\n",
      "Generate random text, e.g., using `random.choice(\"abcdefg \")`, taking care to include the space character. You will need to import random first. Use the string concatenation operator to accumulate characters into a (very) long string. Then tokenize this string, and generate the Zipf plot as before, and compare the two plots. What do you make of Zipf's Law in the light of this?\n",
      "</p>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Loading your own Corpus"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So if a corpus is so useful, why not create your own? How hard could it be? And the answer is... easy! As (almost) everything else in Python!\n",
      "\n",
      "If you have your own collection of text files that you would like to access using the above methods, you can easily load them with the help of NLTK's `PlaintextCorpusReader`. Check the path of your files on your computer. Let's say that we have all we need at `/usr/share/dict`. Whatever the location, set this to be the value of `corpus_root`, the first parameter for `PlaintextCorpusReader`. The second parameter can be a list of `fileids`, like ['a.txt', 'test/b.txt'], or a pattern that matches all `fileids`, like `'[abc]/.*\\.txt'` (you don't know yet about regular expressions, but give it a chance)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import PlaintextCorpusReader\n",
      "corpus_root = '/usr/share/dict'\n",
      "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
      "wordlists.fileids()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "['README.select-wordlist',\n",
        " 'american-english',\n",
        " 'british-english',\n",
        " 'cracklib-small',\n",
        " 'words',\n",
        " 'words.pre-dictionaries-common']"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "wordlists.words('words')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "['A', 'A', \"'\", 's', 'AA', \"'\", 's', 'AB', \"'\", 's', ...]"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It was easy, wasn't it?"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Sources of data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's really nice that NLTK has all those books and corpora ready to use. But the reality is that very often the data you need to analyze is on the Internet, in a blog, a newspaper or it comes from Twitter or Facebook. So let's see some ways to scrape content like that.\n",
      "\n",
      "The most basic library in the Standard Python Library to fetch content from the Internet is `urllib`. There is a `urllib2`, and even a `urllib3`, fortunately in Python 3 eveything is under `urllib`. However, in the past years, more powerful tools have shown up. Probably the best is [`requests`](http://docs.python-requests.org/en/latest/), a package with the moto *HTTP for Humans*. Well, there is no such a thing, but still is really good and intuitive.\n",
      "\n",
      "The most basic example is fetching plain text content."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from urllib.request import urlopen\n",
      "url = \"http://www.gutenberg.org/files/2554/2554.txt\"\n",
      "html = urlopen(url).read()\n",
      "html[:75]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 23,
       "text": [
        "b'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n'"
       ]
      }
     ],
     "prompt_number": 23
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can also fetch content of a website like [Globe and Mail](http://www.theglobeandmail.com/)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "from urllib.request import urlopen\n",
      "html = urlopen(\"http://news.bbc.co.uk/2/hi/health/2284783.stm\").read()\n",
      "html[:300]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "b'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN\" \"http://www.w3.org/TR/REC-html40/loose.dtd\">\\r\\n<html>\\r\\n<head>\\r\\n<title>BBC NEWS | Health | Blondes \\'to die out in 200 years\\'</title>\\r\\n<meta name=\"keywords\" content=\"BBC, News, BBC News, news online, world, uk, international, foreign, britis'"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can filter out only the text. To do so, we remove all HTML tags by using [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/bs4/doc/), which is not necessary to master unless you want to parse HTML, which is not the case. Almost all we need to know so far about BeautifulSoup is how remove HTML tags, and that's done by calling the method `get_text()` after creating an instance of `BeautifulSoup`. A worthwhile reading is [Intro to Beautiful Soup, from The Programming Historian](http://programminghistorian.org/lessons/intro-to-beautiful-soup)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "globe_mail = BeautifulSoup(html).get_text()\n",
      "print(globe_mail[:30])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "BBC NEWS | Health | Blondes \n"
       ]
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And that's pretty much it. From here we can tokenize in words, sentences, or whatever we need to do."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print(nltk.sent_tokenize(globe_mail)[5])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "In order for a child to have blonde hair, it must have the gene on both sides of the family in the grandparents' generation.\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Usually news sites and blogs have what is called sindication, that allows to access the content in cleaner way, avoiding the pain of dealing with HTML as much as possible. This is called a RSS feed. With the help of a third-party Python library called the [Universal Feed Parser](http://feedparser.org/) we can access the content of a blog, as shown below:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import feedparser\n",
      "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
      "llog['feed']['title']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "'Language Log'"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(llog.entries)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 28,
       "text": [
        "13"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "post = llog.entries[2]\n",
      "post.title"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 29,
       "text": [
        "'It&#039;s not easy seeing green'"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "content = post.content[0].value\n",
      "content[:70]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "'<p>The whole <a href=\"http://xkcd.com/1492/\" target=\"_blank\">dress</a>'"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
      "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
      "Activity\n",
      "</p>\n",
      "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
      "Play around with the objects and methods from `feedparser` and `BeautifulSoup`.\n",
      "</p>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Language detection"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Language detection has always been a very difficult task. Althouh it's really important when scrapping and analyzing text in order to filter out only what you are interested in. It is possible to achieve [good results by using just NLTK](has been a very difficult task), but it gets too hard. What we are really looking for is `TextBlob`.\n",
      "\n",
      "`TextBlob` is built on top of NLTK and other library called [pattern](http://www.clips.ua.ac.be/pages/pattern), and one of those task that makes dead-simple for us is language detection. It's so simple, and gives so good results, that it's enough for us.\n",
      "\n",
      "Installing is as simple as running the next command in a terminal:\n",
      "```\n",
      "$ pip install textblob\n",
      "```\n",
      "\n",
      "Or if you are using [anaconda](http://textblob.readthedocs.org/en/latest/install.html#with-conda):\n",
      "\n",
      "```\n",
      "$ conda config --add channels https://conda.binstar.org/sloria\n",
      "$ conda install textblob\n",
      "$ python -m textblob.download_corpora\n",
      "```"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob import TextBlob"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(u\"Simple is better than complex.\").detect_language()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "'en'"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(\"Simple es mejor que complejo.\").detect_language()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 33,
       "text": [
        "'es'"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(u\"\u7f8e\u4e3d\u4f18\u4e8e\u4e11\u964b\").detect_language()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "'zh-CN'"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "TextBlob(u\"\u0628\u0633\u064a\u0637 \u0647\u0648 \u0623\u0641\u0636\u0644 \u0645\u0646 \u0645\u062c\u0645\u0639\").detect_language()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 35,
       "text": [
        "'ar'"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Simply amazing! The language codes, `'en'` for English, `'es'` for Spanish, `zh-CN` for Chinese, or `'ar'` for Arabic, are in [ISO639-1 format](https://en.wikipedia.org/wiki/ISO639-1)."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Machine translation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The other task that `TextBlob` improves is translation. There is not much to say, just take a look to the functions."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "en_blob = TextBlob(u\"Simple is better than complex.\")\n",
      "en_blob.translate(to=\"es\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 36,
       "text": [
        "TextBlob(\"Simple es mejor que complejo .\")"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If no source language is specified, `TextBlob` will attempt to detect the language. You can specify the source language explicitly, like so."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chinese_blob = TextBlob(u\"\u7f8e\u4e3d\u4f18\u4e8e\u4e11\u964b\")\n",
      "chinese_blob.translate(from_lang=\"zh-CN\", to='en')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "TextBlob(\"Beautiful is better than ugly\")"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The translation service is provided by the [Google Translate API](https://developers.google.com/translate/), but please, [don't trust it that much](http://translationparty.com/#11323941). It may work pretty good for short sentences and words, but it's not very reliable in large chunks of text, so tokenize first."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<div style=\"font-size: 1em; margin: 1em 0 1em 0; border: 1px solid #86989B; background-color: #f7f7f7; padding: 0;\">\n",
      "<p style=\"margin: 0; padding: 0.1em 0 0.1em 0.5em; color: white; border-bottom: 1px solid #86989B; font-weight: bold; background-color: #AFC1C4;\">\n",
      "Activity\n",
      "</p>\n",
      "<p style=\"margin: 0.5em 1em 0.5em 1em; padding: 0;\">\n",
      "Write a program that loads feeds from the [Spanish Blog in Digital Humanities](http://humanidadesdigitales.net/blog/feed/), get the first 10 entries using `feedparser`, and for each, returns the next in English and withouth stopwords (*Hint*: take a look to the stopwords in NLTK under `nltk.corpus.stopwords.words('spanish')`):\n",
      "</p>\n",
      "<ul><li>\n",
      "Title\n",
      "</li><li>\n",
      "Number of sentences\n",
      "</li><li>\n",
      "Number of words\n",
      "</li><li>\n",
      "Number of unique words (vocabulary)\n",
      "</li><li>\n",
      "Number of hapaxes\n",
      "</li><li>\n",
      "Top 10 most frequent words\n",
      "</li></ul>\n",
      "<br/>\n",
      "</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "For the [next class](class13.ipynb)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Read [Chapter 5, from Natural Language Processing with Python](http://www.nltk.org/book3/)."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}